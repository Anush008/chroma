{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (22.2.2)\n",
      "Requirement already satisfied: pyarrow in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (9.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (from pyarrow) (1.23.4)\n",
      "Requirement already satisfied: pandas in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (from pandas) (1.23.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (from pandas) (2022.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: duckdb in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (0.5.1)\n",
      "Requirement already satisfied: numpy>=1.14 in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (from duckdb) (1.23.4)\n",
      "Requirement already satisfied: hnswlib in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (0.6.2)\n",
      "Requirement already satisfied: numpy in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (from hnswlib) (1.23.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install pyarrow\n",
    "!pip install pandas\n",
    "!pip install duckdb\n",
    "!pip install hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import duckdb\n",
    "import hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table = pq.read_table('test_data_to_record.parquet')\n",
    "panda_test_table = test_table.to_pandas()\n",
    "# print(panda_test_table.head())\n",
    "\n",
    "train_table = pq.read_table('train_data_to_record.parquet')\n",
    "panda_train_table = train_table.to_pandas()\n",
    "# print(panda_train_table.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record in test\n",
      "[(10000,)]\n",
      "Record in train\n",
      "[(60000,)]\n"
     ]
    }
   ],
   "source": [
    "# print(duckdb.query('''\n",
    "# SELECT COUNT(*)\n",
    "# FROM 'test_data_to_record.parquet'\n",
    "# WHERE metadata.location ILIKE '%San Francisco%';\n",
    "# ''').fetchall())\n",
    "print(\"Record in test\")\n",
    "print(duckdb.query('''\n",
    "SELECT COUNT(*)\n",
    "FROM 'test_data_to_record.parquet'\n",
    ";\n",
    "''').fetchall())\n",
    "print(\"Record in train\")\n",
    "print(duckdb.query('''\n",
    "SELECT COUNT(*)\n",
    "FROM 'train_data_to_record.parquet'\n",
    ";\n",
    "''').fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensionality is: 10\n",
      "total number of elements is: 70000\n",
      "max elements 35000\n",
      "concatted_data 70000\n",
      "[-12.8481102  -15.02010345 -10.95824623 -10.21315765 -16.63255692\n",
      " -18.05376053 -21.12276649   7.73786306 -16.47409248 -10.12139034]\n",
      "Adding first batch of 10000 elements\n",
      "Recall for the first batch: 0.9998 \n",
      "\n",
      "Saving index to 'first_half.bin'\n",
      "\n",
      "Loading index from 'first_half.bin'\n",
      "\n",
      "Adding the second batch of 60000 elements\n",
      "Recall for two batches: 0.9998428571428571 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hnswlib\n",
    "import numpy as np\n",
    "\n",
    "#dim = 16\n",
    "# num_elements = 10000\n",
    "\n",
    "# Generating sample data\n",
    "# data = np.float32(np.random.random((num_elements, dim)))\n",
    "\n",
    "# We split the data in two batches:\n",
    "data1 = data[:num_elements // 2]\n",
    "data2 = data[num_elements // 2:]\n",
    "\n",
    "data1 = panda_test_table['embedding_data'].to_numpy().tolist()\n",
    "data2 = panda_train_table['embedding_data'].to_numpy().tolist()\n",
    "dim = len(data1[0])\n",
    "num_elements = len(data1) + len(data2)\n",
    "print(\"dimensionality is:\", dim)\n",
    "print(\"total number of elements is:\", num_elements)\n",
    "print(\"max elements\", num_elements//2)\n",
    "\n",
    "concatted_data = data1 + data2\n",
    "print(\"concatted_data\", len(concatted_data))\n",
    "\n",
    "# Declaring index\n",
    "p = hnswlib.Index(space='l2', dim=dim)  # possible options are l2, cosine or ip\n",
    "\n",
    "# Initing index\n",
    "# max_elements - the maximum number of elements (capacity). Will throw an exception if exceeded\n",
    "# during insertion of an element.\n",
    "# The capacity can be increased by saving/loading the index, see below.\n",
    "#\n",
    "# ef_construction - controls index search speed/build speed tradeoff\n",
    "#\n",
    "# M - is tightly connected with internal dimensionality of the data. Strongly affects the memory consumption (~M)\n",
    "# Higher M leads to higher accuracy/run_time at fixed ef/efConstruction\n",
    "\n",
    "p.init_index(max_elements=len(data1), ef_construction=100, M=16)\n",
    "\n",
    "# Controlling the recall by setting ef:\n",
    "# higher ef leads to better accuracy, but slower search\n",
    "p.set_ef(10)\n",
    "\n",
    "# Set number of threads used during batch search/construction\n",
    "# By default using all available cores\n",
    "p.set_num_threads(4)\n",
    "\n",
    "print(data1[0])\n",
    "\n",
    "print(\"Adding first batch of %d elements\" % (len(data1)))\n",
    "p.add_items(data1)\n",
    "\n",
    "# Query the elements for themselves and measure recall:\n",
    "labels, distances = p.knn_query(data1, k=1)\n",
    "print(\"Recall for the first batch:\", np.mean(labels.reshape(-1) == np.arange(len(data1))), \"\\n\")\n",
    "\n",
    "# Serializing and deleting the index:\n",
    "index_path='first_half.bin'\n",
    "print(\"Saving index to '%s'\" % index_path)\n",
    "p.save_index(index_path)\n",
    "del p\n",
    "\n",
    "# Reiniting, loading the index\n",
    "p = hnswlib.Index(space='l2', dim=dim)  # the space can be changed - keeps the data, alters the distance function.\n",
    "\n",
    "print(\"\\nLoading index from 'first_half.bin'\\n\")\n",
    "\n",
    "# Increase the total capacity (max_elements), so that it will handle the new data\n",
    "p.load_index(\"first_half.bin\", max_elements = num_elements)\n",
    "\n",
    "print(\"Adding the second batch of %d elements\" % (len(data2)))\n",
    "p.add_items(data2)\n",
    "\n",
    "# Query the elements for themselves and measure recall:\n",
    "labels, distances = p.knn_query(concatted_data, k=1)\n",
    "print(\"Recall for two batches:\", np.mean(labels.reshape(-1) == np.arange(len(concatted_data))), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('chroma_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d9c3dc62c6aa7b00acc049aade78a432d73b02c9ef7ad39c205ab8449029961"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
