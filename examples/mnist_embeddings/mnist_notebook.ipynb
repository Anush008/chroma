{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (22.2.2)\n",
      "Requirement already satisfied: pyarrow in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (9.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (from pyarrow) (1.23.4)\n",
      "Requirement already satisfied: pandas in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (1.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (from pandas) (1.23.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (from pandas) (2022.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: duckdb in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (0.5.1)\n",
      "Requirement already satisfied: numpy>=1.14 in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (from duckdb) (1.23.4)\n",
      "Requirement already satisfied: hnswlib in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (0.6.2)\n",
      "Requirement already satisfied: numpy in /Users/jeff/src/chroma/chroma_env/lib/python3.9/site-packages (from hnswlib) (1.23.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install pyarrow\n",
    "!pip install pandas\n",
    "!pip install duckdb\n",
    "!pip install hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import duckdb\n",
    "import hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      embedding_data  \\\n",
      "0  [-19.19756317138672, -18.10773468017578, -22.2...   \n",
      "1  [6.993745803833008, -16.4917049407959, -12.157...   \n",
      "2  [-13.4398193359375, -6.445910930633545, -7.608...   \n",
      "3  [-15.908609390258789, 7.963416576385498, -12.5...   \n",
      "4  [-16.739404678344727, -16.108036041259766, -20...   \n",
      "\n",
      "                resource_uri                                      metadata  \\\n",
      "0  train-images-idx3-ubyte-0  {'location': 'San Francisco', 'quality': 85}   \n",
      "1  train-images-idx3-ubyte-1         {'location': 'Chicago', 'quality': 0}   \n",
      "2  train-images-idx3-ubyte-2  {'location': 'San Francisco', 'quality': 35}   \n",
      "3  train-images-idx3-ubyte-3   {'location': 'San Francisco', 'quality': 1}   \n",
      "4  train-images-idx3-ubyte-4         {'location': 'Dallas', 'quality': 58}   \n",
      "\n",
      "                                   infer  \\\n",
      "0  {'annotations': [{'category_id': 5}]}   \n",
      "1  {'annotations': [{'category_id': 0}]}   \n",
      "2  {'annotations': [{'category_id': 4}]}   \n",
      "3  {'annotations': [{'category_id': 1}]}   \n",
      "4  {'annotations': [{'category_id': 9}]}   \n",
      "\n",
      "                                   label  \n",
      "0  {'annotations': [{'category_id': 5}]}  \n",
      "1  {'annotations': [{'category_id': 0}]}  \n",
      "2  {'annotations': [{'category_id': 4}]}  \n",
      "3  {'annotations': [{'category_id': 1}]}  \n",
      "4  {'annotations': [{'category_id': 9}]}  \n"
     ]
    }
   ],
   "source": [
    "test_table = pq.read_table('test_data_to_record.parquet')\n",
    "panda_test_table = test_table.to_pandas()\n",
    "# print(panda_test_table.head())\n",
    "\n",
    "train_table = pq.read_table('train_data_to_record.parquet')\n",
    "panda_train_table = train_table.to_pandas()\n",
    "print(panda_train_table.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record in test\n",
      "[(10000,)]\n",
      "Record in train\n",
      "[(60000,)]\n"
     ]
    }
   ],
   "source": [
    "# print(duckdb.query('''\n",
    "# SELECT COUNT(*)\n",
    "# FROM 'test_data_to_record.parquet'\n",
    "# WHERE metadata.location ILIKE '%San Francisco%';\n",
    "# ''').fetchall())\n",
    "print(\"Record in test\")\n",
    "print(duckdb.query('''\n",
    "SELECT COUNT(*)\n",
    "FROM 'test_data_to_record.parquet'\n",
    ";\n",
    "''').fetchall())\n",
    "print(\"Record in train\")\n",
    "print(duckdb.query('''\n",
    "SELECT COUNT(*)\n",
    "FROM 'train_data_to_record.parquet'\n",
    ";\n",
    "''').fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0]\n",
      " [   1]\n",
      " [   2]\n",
      " ...\n",
      " [9997]\n",
      " [9998]\n",
      " [9999]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "import hnswlib\n",
    "import numpy as np\n",
    "\n",
    "#dim = 16\n",
    "# num_elements = 10000\n",
    "\n",
    "# Generating sample data\n",
    "# data = np.float32(np.random.random((num_elements, dim)))\n",
    "\n",
    "# We split the data in two batches:\n",
    "data1 = data[:num_elements // 2]\n",
    "data2 = data[num_elements // 2:]\n",
    "\n",
    "data1 = panda_test_table['embedding_data'].to_numpy().tolist()\n",
    "data2 = panda_train_table['embedding_data'].to_numpy().tolist()\n",
    "dim = len(data1[0])\n",
    "num_elements = len(data1) + len(data2)\n",
    "# print(\"dimensionality is:\", dim)\n",
    "# print(\"total number of elements is:\", num_elements)\n",
    "# print(\"max elements\", num_elements//2)\n",
    "\n",
    "concatted_data = data1 + data2\n",
    "# print(\"concatted_data\", len(concatted_data))\n",
    "\n",
    "# Declaring index\n",
    "p = hnswlib.Index(space='l2', dim=dim)  # possible options are l2, cosine or ip\n",
    "\n",
    "# Initing index\n",
    "# max_elements - the maximum number of elements (capacity). Will throw an exception if exceeded\n",
    "# during insertion of an element.\n",
    "# The capacity can be increased by saving/loading the index, see below.\n",
    "#\n",
    "# ef_construction - controls index search speed/build speed tradeoff\n",
    "#\n",
    "# M - is tightly connected with internal dimensionality of the data. Strongly affects the memory consumption (~M)\n",
    "# Higher M leads to higher accuracy/run_time at fixed ef/efConstruction\n",
    "\n",
    "p.init_index(max_elements=len(data1), ef_construction=100, M=16)\n",
    "\n",
    "# Controlling the recall by setting ef:\n",
    "# higher ef leads to better accuracy, but slower search\n",
    "p.set_ef(10)\n",
    "\n",
    "# Set number of threads used during batch search/construction\n",
    "# By default using all available cores\n",
    "p.set_num_threads(4)\n",
    "\n",
    "# print(data1[0])\n",
    "\n",
    "# print(\"Adding first batch of %d elements\" % (len(data1)))\n",
    "p.add_items(data1)\n",
    "\n",
    "# Query the elements for themselves and measure recall:\n",
    "labels, distances = p.knn_query(data1, k=1)\n",
    "print(labels)\n",
    "\n",
    "print(distances)\n",
    "# print(\"Recall for the first batch:\", np.mean(labels.reshape(-1) == np.arange(len(data1))), \"\\n\")\n",
    "\n",
    "# Serializing and deleting the index:\n",
    "index_path='first_half.bin'\n",
    "# print(\"Saving index to '%s'\" % index_path)\n",
    "p.save_index(index_path)\n",
    "del p\n",
    "\n",
    "# Reiniting, loading the index\n",
    "p = hnswlib.Index(space='l2', dim=dim)  # the space can be changed - keeps the data, alters the distance function.\n",
    "\n",
    "# print(\"\\nLoading index from 'first_half.bin'\\n\")\n",
    "\n",
    "# Increase the total capacity (max_elements), so that it will handle the new data\n",
    "p.load_index(\"first_half.bin\", max_elements = num_elements)\n",
    "\n",
    "# print(\"Adding the second batch of %d elements\" % (len(data2)))\n",
    "p.add_items(data2)\n",
    "\n",
    "# Query the elements for themselves and measure recall:\n",
    "labels, distances = p.knn_query(concatted_data, k=1)\n",
    "# print(\"Recall for two batches:\", np.mean(labels.reshape(-1) == np.arange(len(concatted_data))), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asdf\n"
     ]
    }
   ],
   "source": [
    "print(\"Asdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "panda_test_table                                       embedding_data  \\\n",
      "0  [-12.84811019897461, -15.020103454589844, -10....   \n",
      "1  [-8.334704399108887, -6.763201713562012, 6.999...   \n",
      "2  [-12.62231159210205, 6.12582540512085, -9.9566...   \n",
      "3  [5.2354021072387695, -13.995504379272461, -11....   \n",
      "4  [-11.92990493774414, -14.994053840637207, -12....   \n",
      "\n",
      "               resource_uri                                 metadata  \\\n",
      "0  t10k-images-idx3-ubyte-0   {'location': 'Chicago', 'quality': 26}   \n",
      "1  t10k-images-idx3-ubyte-1   {'location': 'Atlanta', 'quality': 82}   \n",
      "2  t10k-images-idx3-ubyte-2  {'location': 'New York', 'quality': 38}   \n",
      "3  t10k-images-idx3-ubyte-3  {'location': 'New York', 'quality': 67}   \n",
      "4  t10k-images-idx3-ubyte-4   {'location': 'Chicago', 'quality': 92}   \n",
      "\n",
      "                                   infer  \\\n",
      "0  {'annotations': [{'category_id': 7}]}   \n",
      "1  {'annotations': [{'category_id': 2}]}   \n",
      "2  {'annotations': [{'category_id': 1}]}   \n",
      "3  {'annotations': [{'category_id': 0}]}   \n",
      "4  {'annotations': [{'category_id': 4}]}   \n",
      "\n",
      "                                   label  \n",
      "0  {'annotations': [{'category_id': 7}]}  \n",
      "1  {'annotations': [{'category_id': 2}]}  \n",
      "2  {'annotations': [{'category_id': 1}]}  \n",
      "3  {'annotations': [{'category_id': 0}]}  \n",
      "4  {'annotations': [{'category_id': 4}]}  \n"
     ]
    }
   ],
   "source": [
    "def unpack_annotations(embeddings):\n",
    "    # Get and unpack inference data\n",
    "    annotations = [\n",
    "        embedding['infer'][\"annotations\"]\n",
    "        for embedding in embeddings\n",
    "    ]  # Unpack JSON\n",
    "    # print(\"annotations\", annotations)\n",
    "    annotations = [\n",
    "        annotation for annotation_list in annotations for annotation in annotation_list\n",
    "    ]  # Flatten the list\n",
    "    # print(\"annotations\", annotations)\n",
    "\n",
    "    # categories_by_uid = {\n",
    "    #     annotation[\"id\"]: annotation[\"category_id\"] for annotation in annotations\n",
    "    # }\n",
    "    # print(categories_by_uid)\n",
    "\n",
    "    # # Unpack embedding data\n",
    "    embeddings = [embedding[\"embedding_data\"] for embedding in embeddings]\n",
    "\n",
    "    embedding_vectors_by_category = {}\n",
    "    for embedding_annotation_pair in zip(embeddings, annotations):\n",
    "        data = np.array(embedding_annotation_pair[0])\n",
    "        category = embedding_annotation_pair[1]['category_id'] #categories_by_uid[embedding[\"target\"]]\n",
    "        if category in embedding_vectors_by_category.keys():\n",
    "            embedding_vectors_by_category[category] = np.append(\n",
    "                embedding_vectors_by_category[category], data[np.newaxis, :], axis=0\n",
    "            )\n",
    "        else:\n",
    "            embedding_vectors_by_category[category] = data[np.newaxis, :]\n",
    "\n",
    "    return embedding_vectors_by_category\n",
    "\n",
    "# Get the training embeddings. This is the set of embeddings belonging to datapoints of the training dataset, and to the first created embedding set.\n",
    "training_embedding_vectors_by_category = unpack_annotations(panda_train_table.to_dict('records'))\n",
    "\n",
    "# print(\"training_embedding_vectors_by_category\", training_embedding_vectors_by_category)\n",
    "print(\"panda_test_table\", panda_test_table.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing mean and covariance for label category 5\n",
      "category: 5 samples: 5419\n",
      "Computing mean and covariance for label category 0\n",
      "category: 0 samples: 5932\n",
      "Computing mean and covariance for label category 4\n",
      "category: 4 samples: 5845\n",
      "Computing mean and covariance for label category 1\n",
      "category: 1 samples: 6741\n",
      "Computing mean and covariance for label category 9\n",
      "category: 9 samples: 5937\n",
      "Computing mean and covariance for label category 2\n",
      "category: 2 samples: 5953\n",
      "Computing mean and covariance for label category 3\n",
      "category: 3 samples: 6127\n",
      "Computing mean and covariance for label category 6\n",
      "category: 6 samples: 5919\n",
      "Computing mean and covariance for label category 7\n",
      "category: 7 samples: 6279\n",
      "Computing mean and covariance for label category 8\n",
      "category: 8 samples: 5848\n",
      "train-images-idx3-ubyte-0 3.4324716467737963\n",
      "train-images-idx3-ubyte-1 2.208093177145872\n",
      "train-images-idx3-ubyte-2 3.4119725786935735\n",
      "train-images-idx3-ubyte-3 2.4228611160906492\n"
     ]
    }
   ],
   "source": [
    "inv_covs_by_category = {}\n",
    "means_by_category = {}\n",
    "for category, embeddings in training_embedding_vectors_by_category.items():\n",
    "    print(f\"Computing mean and covariance for label category {category}\")\n",
    "\n",
    "    # Compute the mean and inverse covariance for computing MHB distance\n",
    "    print(f\"category: {category} samples: {embeddings.shape[0]}\")\n",
    "    if embeddings.shape[0] < (embeddings.shape[1] + 1):\n",
    "        print(f\"not enough samples for stable covariance in category {category}\")\n",
    "        continue\n",
    "    cov = np.cov(embeddings.transpose())\n",
    "    try:\n",
    "        inv_cov = np.linalg.inv(cov)\n",
    "    except np.linalg.LinAlgError as err:\n",
    "        print(f\"covariance for category {category} is singular\")\n",
    "        continue\n",
    "    mean = np.mean(embeddings, axis=0)\n",
    "    inv_covs_by_category[category] = inv_cov\n",
    "    means_by_category[category] = mean\n",
    "\n",
    "# print(means_by_category[5])\n",
    "target_datapoints = panda_test_table.to_dict('records') + panda_train_table.to_dict('records')\n",
    "# print(target_datapoints)\n",
    "\n",
    "# Process each datapoint's inferences individually. This is going to be very slow.\n",
    "# This is because there is no way to grab the corresponding metadata off the datapoint.\n",
    "# We could instead put it on the embedding directly ?\n",
    "inference_metadata = {}\n",
    "for idx, datapoint in enumerate(target_datapoints):\n",
    "    inferences = datapoint['infer'][\"annotations\"]\n",
    "    embeddings = [datapoint[\"embedding_data\"]]\n",
    "\n",
    "    for i in range(len(inferences)):\n",
    "        emb_data = embeddings[i]\n",
    "        category = inferences[i][\"category_id\"]\n",
    "        if not category in inv_covs_by_category.keys():\n",
    "            continue\n",
    "        mean = means_by_category[category]\n",
    "        inv_cov = inv_covs_by_category[category]\n",
    "        delta = np.array(emb_data) - mean\n",
    "        squared_mhb = np.sum((delta * np.matmul(inv_cov, delta)), axis=0)\n",
    "        if squared_mhb < 0:\n",
    "            print(f\"squared distance for category {category} is negative\")\n",
    "            continue\n",
    "        distance = np.sqrt(squared_mhb)\n",
    "        # print(datapoint)\n",
    "        inference_metadata[datapoint[\"resource_uri\"]] = distance\n",
    "\n",
    "# dict to json\n",
    "import json\n",
    "\n",
    "# quality scores from db verison 0.0.2\n",
    "# 1|{\"distance_score\": 3.0529832292425403}\n",
    "# 2|{\"distance_score\": 2.088222399506212}\n",
    "# 3|{\"distance_score\": 3.7741301182872675}\n",
    "# 4|{\"distance_score\": 2.3315582411124463}\n",
    "# 5|{\"distance_score\": 1.6887465106278159}\n",
    "# 6|{\"distance_score\": 2.1548636869467788}\n",
    "# 7|{\"distance_score\": 3.2068276867648056}\n",
    "# 8|{\"distance_score\": 2.3359545036332805}\n",
    "# 9|{\"distance_score\": 2.4505819331999517}\n",
    "\n",
    "# id|created_at|updated_at|data|input_identifier|inference_identifier|label|embedding_set_id|layer_id|datapoint_id\n",
    "# 1|2022-10-13 16:48:50|2022-10-13 16:48:50|[-19.19756317138672, -18.10773468017578, -22.252187728881836, -1.7129746675491333, -20.99203109741211, 4.6510820388793945, -11.819682121276855, -17.237871170043945, -10.40627384185791, -7.90831184387207]||||1||1\n",
    "# 2|2022-10-13 16:48:50|2022-10-13 16:48:50|[6.993745803833008, -16.4917049407959, -12.157032012939453, -21.126211166381836, -19.217905044555664, -21.072765350341797, -9.723943710327148, -14.83087158203125, -14.961665153503418, -14.060187339782715]||||1||2\n",
    "# 3|2022-10-13 16:48:50|2022-10-13 16:48:50|[-13.4398193359375, -6.445910930633545, -7.608524799346924, -13.623757362365723, 5.8722381591796875, -15.087124824523926, -12.686633110046387, -8.033711433410645, -11.658220291137695, -8.04366683959961]||||1||3\n",
    "# 4|2022-10-13 16:48:50|2022-10-13 16:48:50|[-15.908609390258789, 7.963416576385498, -12.563691139221191, -18.787290573120117, -7.542619705200195, -12.971125602722168, -12.110857963562012, -9.590605735778809, -12.207917213439941, -17.283729553222656]||||1||4\n",
    "# 5|2022-10-13 16:48:50|2022-10-13 16:48:50|[-16.739404678344727, -16.108036041259766, -20.445873260498047, -16.767101287841797, -7.684770584106445, -16.530820846557617, -23.035736083984375, -13.244646072387695, -9.138373374938965, 5.229954719543457]||||1||5\n",
    "# 6|2022-10-13 16:48:50|2022-10-13 16:48:50|[-16.288536071777344, -18.42648696899414, 7.389773368835449, -15.272856712341309, -18.917083740234375, -25.22352409362793, -19.038244247436523, -17.658218383789062, -8.270002365112305, -19.835947036743164]||||1||6\n",
    "# 7|2022-10-13 16:48:50|2022-10-13 16:48:50|[-13.312373161315918, 6.984036922454834, -13.242178916931152, -9.999306678771973, -6.004954814910889, -10.29246997833252, -13.447978973388672, -8.89227294921875, -8.559014320373535, -10.059027671813965]||||1||7\n",
    "# 8|2022-10-13 16:48:50|2022-10-13 16:48:50|[-23.386146545410156, -19.125940322875977, -13.216439247131348, 6.500119686126709, -24.382715225219727, -14.136991500854492, -26.391183853149414, -12.205113410949707, -14.392107009887695, -11.659762382507324]||||1||8\n",
    "# 9|2022-10-13 16:48:50|2022-10-13 16:48:50|[-13.129637718200684, 7.2978363037109375, -15.127612113952637, -10.571370124816895, -4.537073135375977, -8.433134078979492, -12.171133041381836, -7.148005485534668, -9.562264442443848, -9.543116569519043]||||1||9\n",
    "\n",
    "\n",
    "# print(len(inference_metadata))\n",
    "print(\"train-images-idx3-ubyte-0\", (inference_metadata[\"train-images-idx3-ubyte-0\"]))\n",
    "print(\"train-images-idx3-ubyte-1\", (inference_metadata[\"train-images-idx3-ubyte-1\"]))\n",
    "print(\"train-images-idx3-ubyte-2\", (inference_metadata[\"train-images-idx3-ubyte-2\"]))\n",
    "print(\"train-images-idx3-ubyte-3\", (inference_metadata[\"train-images-idx3-ubyte-3\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('chroma_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d9c3dc62c6aa7b00acc049aade78a432d73b02c9ef7ad39c205ab8449029961"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
