{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query relevance using Chroma density\n",
    "\n",
    "Density is a new experimental feature in Chroma. The aim is to give LM-in-the-loop application developers an easy way \n",
    "to evaluate whether a given input is supported by the data in the embedding store. \n",
    "\n",
    "For an in-depth writeup and more example use-cases, see [this post]().\n",
    "\n",
    "**We value feedback on this experimental feature! [Join us in Discord](https://discord.gg/9WZAkTEEwC)** \n",
    "\n",
    "## Relevant queries\n",
    "\n",
    "Simply retrieving a number of 'most relevant' documents is often insufficient, because while they might be the _most_ \n",
    "relevant, it's not necessarily true that they're _actually_ relevant to the query. \n",
    "\n",
    "The simplest way to deal with this is to restrict results by certain distance threshold, for example a maximum cosine\n",
    "distance between the query and the retrieved results. \n",
    "\n",
    "However, what this threshold should be will vary by dataset and embedding model. Finding the right threshold might \n",
    "require significant tuning and experimentation for each new dataset, and the right threshold is likely to vary \n",
    "with the data. \n",
    "\n",
    "## Just ask the model ?\n",
    "\n",
    "One approach to this problem is to just ask the model whether information in its context is relevant to the query.\n",
    "This might work well in the general case, but comes with some drawbacks. \n",
    "\n",
    "LM calls are relatively expensive compared to querying Chroma. Additionally, in many use-cases, the model may not be \n",
    "able to assess the relevancy of a document to a given query, especially if there are few or no examples of these types \n",
    "of documents in the training corpus. \n",
    "\n",
    "## Algorithmic relevance \n",
    "\n",
    "Density is an algorithmic approach to the problem of query relevance, which automatically adapts to the dataset and\n",
    "embedding model. In brief, Chroma computes a distance percentile for each query result and returns it as an auxilliary \n",
    "output along with the query result itself. Here's how it works: \n",
    "\n",
    "- For each embedding in the dataset, we compute the distances to N nearest neighbors. \n",
    "- We construct a cumulative density over the returned distances, and store it as a lookup table. \n",
    "\n",
    "When a query arrives:\n",
    "\n",
    "- We get the distance to the query for each result.\n",
    "- We use the cumulative density lookup table to determine the percentile each distance lies in relative to the dataset \n",
    "as a whole.\n",
    "- We return this percentile $p$ as an auxilliary output. \n",
    "\n",
    "The percentile $p$ can be thought of as a proxy for the probability that a given neighbor is relevant to the query. We\n",
    "can then use this proxy value to decide whether a result is relevant before giving it to the LM.\n",
    "\n",
    "See [this post]() for a more in-depth write up. On to the examples! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "import chromadb.experimental\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "As an example dataset we use an embedding of the bible. \n",
    "\n",
    "We embed the bible is by verse, using the `multi-qa-mpnet-base-dot-v1` embedding model \n",
    "from [Sentence Transformers](https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1). This model is \n",
    "trained such that short questions and their accompanying answers lie close together in emebedding space, \n",
    "under the dot product. \n",
    "\n",
    "For convenience, we supply the dataset as a pre-made Chroma index, and load it using a persisted client. \n",
    "The metadata contains information about the chapter, verse, and book the verse is from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the chroma-bible persistent directory exists\n",
    "persist_directory = \"chroma-bible\"\n",
    "\n",
    "if not os.path.exists(persist_directory):\n",
    "\n",
    "    # Download the chroma-bible index from https://chroma-datastore.sfo3.digitaloceanspaces.com/chroma-bible.zip\n",
    "    urllib.request.urlretrieve(\"https://chroma-datastore.sfo3.digitaloceanspaces.com/chroma-bible.zip\", \"chroma-bible.zip\")\n",
    "\n",
    "    # Unzip the chroma-bible persistant directory\n",
    "    with zipfile.ZipFile(\"chroma-bible.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "We load the data using the Chroma experimental client, which gives us access to the density feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     ###### \u001b[38;5;220m Welcome to the Chroma experimental client! \u001b[0m #####\n",
      "     \n",
      "                \u001b[38;5;069m(((((((((    \u001b[38;5;203m(((((\u001b[38;5;220m####         \n",
      "             \u001b[38;5;069m(((((((((((((\u001b[38;5;203m(((((((((\u001b[38;5;220m#########    \n",
      "           \u001b[38;5;069m(((((((((((((\u001b[38;5;203m(((((((((((\u001b[38;5;220m###########  \n",
      "         \u001b[38;5;069m((((((((((((((\u001b[38;5;203m((((((((((((\u001b[38;5;220m############ \n",
      "        \u001b[38;5;069m(((((((((((((\u001b[38;5;203m((((((((((((((\u001b[38;5;220m#############\n",
      "        \u001b[38;5;069m(((((((((((((\u001b[38;5;203m((((((((((((((\u001b[38;5;220m#############\n",
      "         \u001b[38;5;069m((((((((((((\u001b[38;5;203m(((((((((((((\u001b[38;5;220m##############\n",
      "         \u001b[38;5;069m((((((((((((\u001b[38;5;203m((((((((((((\u001b[38;5;220m############## \n",
      "           \u001b[38;5;069m((((((((((\u001b[38;5;203m(((((((((((\u001b[38;5;220m#############   \n",
      "             \u001b[38;5;069m((((((((\u001b[38;5;203m((((((((\u001b[38;5;220m##############     \n",
      "                \u001b[38;5;069m(((((\u001b[38;5;203m((((    \u001b[38;5;220m#########\u001b[0m            \n",
      "\n",
      "    \n",
      "\n",
      "We'd love to know what you're experimenting with! Join the Discord:\n",
      "\n",
      "               https://discord.gg/9WZAkTEEwC\n",
      "\n",
      "    \n",
      "Running Chroma using direct local API.\n",
      "loaded in 31102 embeddings\n",
      "loaded in 1 collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antontroynikov/miniforge3/envs/chroma/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "client = chromadb.experimental.ExperimentalClient(\n",
    "    Settings(persist_directory=persist_directory, chroma_db_impl=\"duckdb+parquet\")\n",
    ")\n",
    "collection = client.get_collection(\"bible\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the same embedding function we used to originally create the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddingFunction(model_name=\"multi-qa-mpnet-base-dot-v1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together we define a function for querying the Bible collection, and printing out the \n",
    "results along with their distance percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_bible(query: str, n_results: int = 3):\n",
    "    query_emb = embedding_function([query])\n",
    "    result, distance_percentiles = collection.query(query_embeddings=query_emb, n_results=n_results)\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Results:\")\n",
    "    for documents, distances in zip(result['documents'], distance_percentiles):\n",
    "        for document, distance in zip(documents, distances):\n",
    "            print(f\"\\t{document}\")\n",
    "            print(f\"\\tPercentile: {distance}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queries\n",
    "\n",
    "Let's run some queries. First, let's start with an irrelevant question, which we can use as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who was the 35th president of the United States?\n",
      "Results:\n",
      "\tI said in my haste, All men are liars.\n",
      "\tPercentile: 0.07713452280821043\n",
      "\tAnd again he denied with an oath, I know not the man.\n",
      "\tPercentile: 0.02268830866694105\n",
      "\tBehold, I have told you beforehand.\n",
      "\tPercentile: 0.01549777197158142\n"
     ]
    }
   ],
   "source": [
    "query_bible(\"Who was the 35th president of the United States?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, each of these results has a relatively low distance percentile - they aren't likely relevant to the query.\n",
    "\n",
    "Next, let's try something something that might be covered in the bible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the meaning of life?\n",
      "Results:\n",
      "\tThe Spirit of God hath made me, And the breath of the Almighty giveth me life.\n",
      "\tPercentile: 0.5794090803342937\n",
      "\tHe that loveth his life loseth it; and he that hateth his life in this world shall keep it unto life eternal.\n",
      "\tPercentile: 0.5794090803342937\n",
      "\tAnd this is life eternal, that they should know thee the only true God, and him whom thou didst send, [even] Jesus Christ.\n",
      "\tPercentile: 0.539043289965819\n"
     ]
    }
   ],
   "source": [
    "query_bible(\"What is the meaning of life?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are somewhat more relevant. Though their content doesn't immediately answer the query, we might supply \n",
    "these results to an LM to synthesisze an answer for us. \n",
    "\n",
    "Next lets ask more direct questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How old was Noah when he built the ark?\n",
      "Results:\n",
      "\tAnd all the days of Noah were nine hundred and fifty years: And he died.\n",
      "\tPercentile: 0.841675763703717\n",
      "\tAnd Noah was six hundred years old when the flood of waters was upon the earth.\n",
      "\tPercentile: 0.841675763703717\n",
      "\tAnd Noah lived after the flood three hundred and fifty years.\n",
      "\tPercentile: 0.7610658744787846\n",
      "Query: What is the number of wives of Solomon?\n",
      "Results:\n",
      "\tAnd he had seven hundred wives, princesses, and three hundred concubines; and his wives turned away his heart.\n",
      "\tPercentile: 0.6943646092449758\n",
      "\tAnd there were born unto him seven sons and three daughters.\n",
      "\tPercentile: 0.6190339841443118\n",
      "\tThe proverbs of Solomon the son of David, king of Israel:\n",
      "\tPercentile: 0.6190339841443118\n"
     ]
    }
   ],
   "source": [
    "query_bible(\"How old was Noah when he built the ark?\")\n",
    "query_bible(\"What is the number of wives of Solomon?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get pretty relevant results! \n",
    "Noah's age is given at various points, though only the second result actually answers the query. We also correctly get \n",
    "the number of Solomon's wives (seems he was prett prolific, at least according to scripture).\n",
    "\n",
    "Interestingly, despite both being direct questions about numbers, the percentiles for the results about Solomon\n",
    "are lower, and indeed, only one result is actually relevant. \n",
    "\n",
    "Next, lets ask some questions which aren't just about a single quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Which Apostle took the Gospel to the city of Samaria?\n",
      "Results:\n",
      "\tNow when the apostles that were at Jerusalem heard that Samaria had received the word of God, they sent unto them Peter and John:\n",
      "\tPercentile: 0.9238139551530986\n",
      "\tAnd when they had preached the gospel to that city, and had made many disciples, they returned to Lystra, and to Iconium, and to Antioch,\n",
      "\tPercentile: 0.8628536659567996\n",
      "\tAnd Philip went down to the city of Samaria, and proclaimed unto them the Christ.\n",
      "\tPercentile: 0.8628536659567996\n",
      "Query: What did the apostle Simon do for a living?\n",
      "Results:\n",
      "\tand Simon he surnamed Peter;\n",
      "\tPercentile: 0.841675763703717\n",
      "\tPaul, an apostle of Christ Jesus through the will of God, to the saints that are at Ephesus, and the faithful in Christ Jesus:\n",
      "\tPercentile: 0.4990139408364501\n",
      "\tPaul, an apostle (not from men, neither through man, but through Jesus Christ, and God the Father, who raised him from the dead),\n",
      "\tPercentile: 0.4990139408364501\n"
     ]
    }
   ],
   "source": [
    "query_bible(\"Which Apostle took the Gospel to the city of Samaria?\")\n",
    "query_bible(\"What did the apostle Simon do for a living?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results we get are mixed. For the query regarding Samaria, we get pretty relevant results, though one of them seems\n",
    "to be relevant only because it mentions several cities.\n",
    "\n",
    "For the query regarding Simon, we don't get any results relevant to the actual query, and a high percentile for the \n",
    "first result - it does mention Simon, but not his profession. \n",
    "\n",
    "The reason for this might be two-fold;\n",
    "\n",
    "- Simon's profession might never be in the same verse as his name. Without this context, it's not possible to just \n",
    "answer this query. \n",
    "\n",
    "- The embedding model we used might weight matching entities (e.g. names) more strongly than matching other features,\n",
    "leading to things being similar because they're both cities, instead of their role in the actual question.\n",
    "\n",
    "There is still a lot to explore here - results like these suggest that density can be used not just to estimate \n",
    "query relevance, but also to explore the structure of the dataset for a given embedding model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Density provides a way to estimate the relevancy of query results returned by Chroma which automatically adapts to the \n",
    "dataset and embedding model. We hope it provides you with another tool to build robust AI applications.\n",
    "\n",
    "There's a lot more work to do to answer questions around how text should be chunked, tokenized, embeeded and retrieved.\n",
    "If you're interested in working with us on these and other problems, (we are hiring!)[https://www.notion.so/trychroma/careers-chroma-9d017c3007c7478ebd85bad854101497].\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chroma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88f09714c9334832bac29166716f9f6a879ee2a4ed4822c1d4120cb2393b58dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
